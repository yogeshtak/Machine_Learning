{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e683fdb4",
   "metadata": {},
   "source": [
    "# Building Your NLP Vocabulary\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "In order to build up a vocabulary, the first thing to do is to break the documents or sentences into chunks called tokens. Each token carries a semantic meaning associated with it. Tokenization is one of the fundamental things to do in any text-processing activity. Tokenization can be thought of as a segmentation technique wherein you are trying to break down larger pieces of text chunks into smaller meaningful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a3ce40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'capital', 'of', 'India', 'is', 'Delhi']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The capital of India is Delhi\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f00aa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"India's\", 'capital', 'is', 'Delhi']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"India's capital is Delhi\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15a2f5",
   "metadata": {},
   "source": [
    "In the preceding example, should it be India, Indias, or India's? A split method does\n",
    "not often know how to deal with situations containing apostrophes.\n",
    "\n",
    "### Different types of tokenizers\n",
    "1. **Regular expression based tokenizer**\n",
    "2. **Treebank tokenizer**\n",
    "3. **TweetTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c1b2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'costs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " '$3000.0',\n",
       " '-',\n",
       " '$8000.0',\n",
       " 'in',\n",
       " 'the',\n",
       " 'USA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "sentence = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in the USA\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[d\\.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddaabd1",
   "metadata": {},
   "source": [
    "The `\\w+|\\$[\\d\\.]+|\\S+` regular expression allows three alternative patterns:\n",
    "\n",
    "**First alternative:** `\\w+` that matches any word character (equal to [a-zA-Z0-9_]). The + is a quantifier and matches between one and unlimited times as many times as possible. <br>\n",
    "**Second alternative:** `\\$[\\d\\.]+`.  Here, `\\$` matches the character $, \\d matches a digit between 0 and 9, \\. matches the character . (period), and + again acts as a quantifier matching between one and unlimited times. <br>\n",
    "**Third alternative:** `\\S+`. Here, \\S accepts any non-whitespace character and + again acts the same way as in the preceding two alternatives.\n",
    "\n",
    "There are other tokenizers built on top of the RegexpTokenizer, such as the BlankLine tokenizer, which tokenizes a string treating blank lines as delimiters where blank lines are those that contain no characters except spaces or tabs.\n",
    "The WordPunct tokenizer is another implementation on top of RegexpTokenizer, which tokenizes a text into a sequence of alphabetic and nonalphabetic characters using the regular expression \\w+|[^\\w\\s]+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3da9cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "BlanklineTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62672d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "\n",
    "regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc061558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3',\n",
       " '.',\n",
       " '88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "369bd89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blankline_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9180d2",
   "metadata": {},
   "source": [
    "**The Treebank tokenizer** does a great job of splitting contractions such as doesn't to does and n't. It further identifies periods at the ends of lines and eliminates them. Punctuation such as commas is split if followed by whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "718c8b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'cost',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '3000.0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4be1c",
   "metadata": {},
   "source": [
    "the rise of social media has given rise to an informal language wherein people tag each other using their social media handles and use a lot of emoticons, hashtags, and abbreviated text to express themselves. We need tokenizers in place that can parse such text and make things more understandable. **TweetTokenizer** caters to this use case significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "536597b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@amankedia',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxxxxxxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ecae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce length of Rolexxxxxxx\n",
    "tokenizer = TweetTokenizer(strip_handles=True,\n",
    "                           reduce_len = True)\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b060d24",
   "metadata": {},
   "source": [
    "The parameter strip_handles, when set to True, removes the handles mentioned in a post/tweet. As can be seen in the preceding output, @amankedia is stripped, since it is a handle.\n",
    "\n",
    "One more parameter that is available with TweetTokenizer is preserve_case, which, when set to False, converts everything to lower case in order to normalize the vocabulary. The default value for this parameter is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e0296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
